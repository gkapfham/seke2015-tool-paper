{\bf Automatic doubling}.
    \toolname \cite{kinneer2015,tool} is a tool to derive an
    ``\underline{EXP}erimental big\underline{O}h'' for supporting
    ``\underline{S}calability \underline{E}valuation''.
    \toolname infers an algorithm's big-Oh order of growth by
    conducting a doubling experiment automatically.  In order to
    evaluate an algorithm $A$, \toolname takes as input two
    functions. The first is a timing function $f(n)$ that runs an
    implementation of $A$ on the provided input $n$ and returns
    the runtime, and the second is a doubling function $d(n)$ that
    accepts an input for $A$ and returns an input $2n$ twice as
    large. After providing \toolname an initial input $n$, the tool
    will output $A$'s big-Oh order of growth.

    \toolname infers the worst-case time complexity of $A$ by
    repeatedly doubling $n$ until $n$ is large enough that the
    worst-case time complexity of $A$ is apparent.  \toolname
    determines when $n$ is large enough by monitoring the doubling
    ratio $\frac{f(2n)}{f(n)}$ for multiple iterations of doubling.
    Using a convergence algorithm, \toolname stops doubling $n$
    when the doubling ratio reaches a stable value.
   
  % Blatantly copied from conference paper
  To test for convergence, for every time $t$, where $t$ denotes the number of times the input has been doubled, we
  record the doubling ratio $r_t = \frac{f(2^t n)}{f(2^{t-1}n)}$. The current ratio $r_c$ is compared to a previous
  ratio $r_p$ where $p$ is determined by a $\mathit{lookback}$ value, such that $p=c-\mathit{lookback}$.  The result of
  the comparison is a $\mathit{difference}$ value, given by $\mathit{difference} = |r_c - r_p|$.  This is then compared
  to a $\mathit{tolerance}$ value, and the experiment is judged to have converged when $\mathit{difference}<\mathit{tolerance}$.
  The $\mathit{lookback}$ and $\mathit{tolerance}$ values are both configurable parameters.

  % Blatantly copied from conference paper with slight changes
  Early use of the tool revealed that this converge checking rule was
  not enough, since a very small initial $n$ may complete nearly
  instantaneous even for multiple rounds of doubling.  For example, the
  time that it takes to sort a list of size $1, 2, 4, 8, \dots 128$
  might not even be distinguishable. This would appear to converge to 1, which 
  indicates constant time complexity. To prevent the experiment from incorrectly terminating
  given a small starting $n$, \toolname requires that a program under study display a ratio of 1 for a
  $\mathit{minimum}$ number of times before judging that the ratio does in fact converge to 1.  That is, if $r_c = 1$,
  $t > \mathit{minimum}$ must be true, in addition to the tolerance test, before the experiment is declared convergent.
  The $\mathit{minimum}$ value is also a configurable parameter.  Because a doubling ratio of 1 signifies
  constant or logarithmic time complexity, requiring these doubles does not significantly increase the experimentation
  time needed, all the while providing further assurance that a small ratio is not due to an insufficiently small $n$.


