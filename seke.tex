%!TEX root=seke.tex
% mainfile: seke.tex

\documentclass[10pt,twocolumn]{article}

\usepackage[table]{xcolor}% http://ctan.org/pkg/xcolor
\usepackage{latex8}
\usepackage{times}
\usepackage{inconsolata}
\usepackage{pifont}

\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{tikz}
\usetikzlibrary{shapes,arrows,shadows}
\usepackage{amsmath,bm,times}
\usepackage{verbatim}

\usepackage{subcaption}
\usepackage{bibspacing}
\usepackage{listings}

\usepackage{xspace}

\usepackage[small,compact]{titlesec}
% \usepackage{titlesec}

\newcommand{\goallegheny}{$^{\mbox{\footnotesize \ding{72}}}$}
\newcommand{\gosheffield}{$^{\mbox{\footnotesize \ding{73}}}$}
\newcommand{\gospace}{$\;$}
\newcommand{\toolname}{{\sc expOse}\xspace}


\begin{document}
\lstset{language=Java}

\title{\vspace*{-.6in}\toolname: Inferring Worst-case Time Complexity by
Automatic Empirical Study\vspace*{-.1in}}

\author{Cody Kinneer \goallegheny                \and
        Gregory M.\ Kapfhammer \goallegheny      \and
        Chris Wright \gosheffield                \and
        Phil McMinn \gosheffield \vspace*{-.1in}
      }

\affiliation{
      \goallegheny \gospace Allegheny College       \and
      \gosheffield \gospace University of Sheffield
}

\maketitle

%TODO cite the main paper (somewhere)

  % Blatantly copied from conference paper
  {\bf Introduction to doubling}. A useful understanding of an algorithm's efficiency, the worst-case time complexity
gives an upper bound on how an increase in the size of the input, denoted $n$, increases the execution time of the
algorithm, $f(n)$.  This relationship is often expressed in the ``big-Oh'' notation, where $f(n)$ is $O(g(n))$ means
that the time increases by no more than on order of $g(n)$. Since the worst-case complexity of an algorithm is evident
when $n$ is large~\cite{mcgeoch2012}, one approach for determining the big-Oh complexity of an algorithm is to conduct
a doubling experiment with increasingly bigger input sizes. By measuring the time needed to run the algorithm on an
input of size $n$ and the time needed to run with input of size $2n$, the algorithm's order of growth can be
empirically determined~\cite{mcgeoch2012}.

The goal of a doubling experiment is to draw a conclusion regarding the efficiency of the algorithm from the ratio
$f(2n)/f(n)$ that represents the factor of change in runtime from input sizes $n$ to $2n$. For instance, a ratio of $2$
would indicate that doubling the input size resulted in the runtime's doubling, thus leading to the conclusion that the
algorithm under study is $O(n)$ or $O(n\log n)$.  Table~\ref{table:ratios} shows some common time complexities and their
corresponding ratios.

\begin{table}[h!]
  \vspace*{-.10in}
  \begin{center}
    \begin{tabular}{c|l}
      Ratio $f(2n)/f(n)$ & Worst-Case Conclusion              \\ \hline
      1                  & constant or logarithmic \\
      2                  & linear or linearithmic  \\
      4                  & quadratic               \\
      8                  & cubic                   \\
      % x                & $O(n^{\log x})$
    \end{tabular}
  \end{center}
  \vspace*{-.25in}

  \caption{Conclusions for worst-case time complexity.}~\label{table:ratios}
  \vspace*{-.30in}

\end{table}

{\bf Automatic doubling}.
    \toolname \cite{kinneer2015,tool} is a tool to derive an
    ``\underline{EXP}erimental big\underline{O}h'' for supporting
    ``\underline{S}calability \underline{E}valuation''.
    \toolname infers an algorithm's big-Oh order of growth by
    conducting a doubling experiment automatically.  In order to
    evaluate an algorithm $A$, \toolname takes as input two
    functions. The first is a timing function $f(n)$ that runs an
    implementation of $A$ on the provided input $n$ and returns
    the runtime, and the second is a doubling function $d(n)$ that
    accepts an input for $A$ and returns an input $2n$ twice as
    large. After providing \toolname an initial input $n$, the tool
    will output $A$'s big-Oh order of growth.

    \toolname infers the worst-case time complexity of $A$ by
    repeatedly doubling $n$ until $n$ is large enough that the
    worst-case time complexity of $A$ is apparent.  \toolname
    determines when $n$ is large enough by monitoring the doubling
    ratio $\frac{f(2n)}{f(n)}$ for multiple iterations of doubling.
    Using a convergence algorithm, \toolname stops doubling $n$
    when the doubling ratio reaches a stable value.
   
  % Blatantly copied from conference paper
  To test for convergence, for every time $t$, where $t$ denotes the number of times the input has been doubled, we
  record the doubling ratio $r_t = \frac{f(2^t n)}{f(2^{t-1}n)}$. The current ratio $r_c$ is compared to a previous
  ratio $r_p$ where $p$ is determined by a $\mathit{lookback}$ value, such that $p=c-\mathit{lookback}$.  The result of
  the comparison is a $\mathit{difference}$ value, given by $\mathit{difference} = |r_c - r_p|$.  This is then compared
  to a $\mathit{tolerance}$ value, and the experiment is judged to have converged when $\mathit{difference}<\mathit{tolerance}$.
  The $\mathit{lookback}$ and $\mathit{tolerance}$ values are both configurable parameters.

  % Blatantly copied from conference paper with slight changes
  Early use of the tool revealed that this converge checking rule was
  not enough, since a very small initial $n$ may complete nearly
  instantaneous even for multiple rounds of doubling.  For example, the
  time that it takes to sort a list of size $1, 2, 4, 8, \dots 128$
  might not even be distinguishable. This would appear to converge to 1, which 
  indicates constant time complexity. To prevent the experiment from incorrectly terminating
  given a small starting $n$, \toolname requires that a program under study display a ratio of 1 for a
  $\mathit{minimum}$ number of times before judging that the ratio does in fact converge to 1.  That is, if $r_c = 1$,
  $t > \mathit{minimum}$ must be true, in addition to the tolerance test, before the experiment is declared convergent.
  The $\mathit{minimum}$ value is also a configurable parameter.  Because a doubling ratio of 1 signifies
  constant or logarithmic time complexity, requiring these doubles does not significantly increase the experimentation
  time needed, all the while providing further assurance that a small ratio is not due to an insufficiently small $n$.

    % TODO fix informal language here
    % the tone I would use when presenting the demo, not sure if
    % appropriate for the tool paper
    {\bf Implementation}.
    \toolname is implemented as an abstract class in the Java
    programming language.  To use \toolname to evaluate your own
    algorithm $A$, you need only extend the \textit{DoublingExperiment}
    class to provide your own $f$ and $d$ functions.  $f$ should be
    implemented by providing a \texttt{double timedTest()} method,
    and $d$ should be implemented by providing a \texttt{void doubleN()}
    method. Note that these methods do not accept any parameters,
    and only \texttt{timedTest()} returns a value. The programmer
    must ensure that \texttt{timedTest()} returns the runtime for the
    current input size, and that when \texttt{doubleN()} is called,
    the input size is doubled.  Actually initializing and storing the
    input should be handled by the specific implementation. The
    \texttt{runExperiment()} method can then be called to to conduct
    a doubling experiment, and the \texttt{printBigOh()} method can
    be called to show the result. Figure~\ref{fig:qsprogram} shows an
    implementation that conducts a doubling experiment on the QuickSort
    algorithm.

    %TODO GMK: do we need to cite the quicksort implementation anywhere?

    % we can cut this, but it does show how easy the tool makes things
    
    \begin{figure}[t]
    \lstinputlisting[basicstyle=\scriptsize]{content/QuickSortTrimmed.java}
    \vspace{-0.15in}
    \caption{A simple Java class that performs a performance evaluation
    on the QuickSort algorithm.}\vspace{-0.20in}
    \label{fig:qsprogram}
    \end{figure}

    {\bf Case study: sorting algorithms}.
    \lstset{language=bash}
    Included with the \toolname tool is an example doubling
    experiment called \texttt{SortingExperiment}.  This program provides a number
    of canonical sorting algorithms with well-known worst-case time
    complexities.  Doubling experiments may be run on these algorithms
    by running the command \texttt{java SortingExperiment algname},
    replacing \texttt{algname} with the name of the desired sorting
    algorithm. Running the command without providing \texttt{algname}
    will show a list of options. When run 1000 times for each of the
    five provided sorting algorithms, \toolname achieves $98.84\%$
    accuracy.

    {\bf Case study: \textit{SchemaAnalyst}}
    In other work \cite{kinneer2015}, we used \toolname to perform a
    comprehensive analysis on the search-based test data generation 
    tool \textit{SchemaAnalyst} \cite{kapfhammer2013} that generates
    test suites for relational database schemas. Since this tool is much 
    more complicated than a sorting algorithm, performing doubling experiments on
    \textit{SchemaAnalyst} requires more parameters.  To conduct these
    experiments, we developed a class called \texttt{SchemaExperiment}
    that extends \texttt{DoublingExperiment}.  We developed 
    \texttt{SchemaExperiment} to allow for conducting doubling
    experiments using a variety of \textit{SchemaAnalyst} configurations,
    as well as accessing \toolname's parameters.

{\scriptsize
\begin{verbatim}
 Usage: <java SchemaExperiment> [options]
Options:  
 -schema, -s        Select which schema to use 
 -criterion         Select which criterion to use 
 -datagenerator     Select which data generator to use 
 -doubler           Select which schemaDoubler to use 
 -convergence       Experiment convergent if differance < tolerance 
 -lookBack          Number of ratios to compare for convergence 
 -tuningTries       Minimum number of times to doubles before O(1)  
 -minDoubles        Minimum number of doubles to try 
 -giveUp, -maxTime  Max time for a single trial in hours 
 -help, -usage      Display command line options 
 -o, -out, -csv     Name of the csv file to save data to 
 -verbose, -debug   Display verbose output 
\end{verbatim}
}

    A doubling experiment that evaluates \textit{SchemaAnalyst} using
    the AICC coverage criterion, random data generator, RiskIt as the initial input
    schema, and according to the number of \textsc{NotNull}s present in the
    schema can be run by the following command:
    \texttt{java SchemaExperiment -criterion AICC -datagenerator random
    -schema RiskIt -doubler DoubleNotNullsSemantic}.

    % is parallel co
{\bf Deploying on an HPC cluster}.
Since the performance of \textit{SchemaAnalyst} may depend on a number
of factors (i.e., criterion, data generator, schema, doubling strategy) a
comprehensive survey of the parameter space may be conducted by
performing a doubling experiment for each configuration. While
computationally expensive, an experiment of this scale is possible by
using an HPC cluster. Each doubling experiment can be run independently
on a separate node of the cluster, and the resulting data can be combined
for analysis. Data mining techniques can then be leveraged to interpret
the performance trade-offs.


 {\bf Parameter Tuning}.
  While \toolname greatly eases the process of conducting
  doubling experiments, its accuracy and performance is
  sensitive to the settings of the system's parameters.  In particular,
  the $\mathit{tolerance}$ and $\mathit{lookback}$ values can result in
  a doubling experiment terminating prematurely or continuing
  indefinitely.
  To complicate the issue further, the parameters must be re-tuned based
  on hardware abilities of the machine(s) being used and the performance
  characteristics of the implementation being studied.

  The reliability of the tool and repeatability of results would be
  improved if \toolname could select good settings for these
  parameters automatically. A reasonable parameter tuning strategy could
  be to run \toolname on various algorithms of known worst-case
  time complexities (such as the sorting algorithms) and lower the
  \textit{tolerance} threshold until \toolname is able to reliably 
  infer the big-Oh complexity of the algorithms.

{\bf Future Work and Conclusion}.
  While we recently used \toolname to study search-based test data
  generation in the domain of relational database
  schemas~\cite{kinneer2015}, the tool is
  general and can be applied to many other problems. Future work
  includes using \toolname to evaluate test data generation for Java
  programs.

  % TODO should I cite EvoSuite?

  \toolname makes empirically evaluating the worst-case time
  complexity of algorithms much more convenient. By automating the
  process of conducting these experiments, \toolname enables large-scale
  empirical studies that would otherwise be infeasible.

% \titleformat{\chapter}{\huge\bf}{}{10pt}{\arabic{chapter} \ }[\titlerule]
\setlength{\bibitemsep}{.075in}
{\footnotesize
  \bibliographystyle{IEEEtran}
% \vspace*{-.05in}
\bibliography{bibtex/seke}}

\end{document}
